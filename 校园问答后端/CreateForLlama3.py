import os
import re
from typing import List, Dict

from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredFileLoader,
    DirectoryLoader
)

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter
)

from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
import argparse


class NWUKnowledgeTrainer:
    def __init__(self):
        self.embeddings = OllamaEmbeddings(model="llama3:8b")
        self.exclude_files = ['.DS_Store', 'Thumbs.db']  # æ’é™¤ç³»ç»Ÿæ–‡ä»¶

        #å®šä¹‰å„ç›®å½•çš„å¤„ç†é…ç½®
        self.category_config = {
            "åŸ¹å…»æ–¹æ¡ˆç›¸å…³": {
                "chunk_size": 1500,
                "chunk_overlap": 400,
                "loader": PyPDFLoader,
                "file_types": [".pdf"]
            },
            "æ—¥å¸¸ç”Ÿæ´»ç›¸å…³": {
                "chunk_size": 800,
                "chunk_overlap": 150,
                "loader": Docx2txtLoader,
                "file_types": [".docx", ".pdf"]
            },
            "ç«èµ›ç›¸å…³": {
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "loader": Docx2txtLoader,
                "file_types": [".pdf"]
            },
            "è¯¾ç¨‹ã€è€ƒè¯•èµ„æºç›¸å…³": {
                "chunk_size": 1200,
                "chunk_overlap": 300,
                "loader": PyPDFLoader,
                "file_types": [".pdf", ".docx"]
            },
            "é€‰è¯¾è€ƒè¯•ç›¸å…³": {
                "chunk_size": 1000,
                "chunk_overlap": 250,
                "loader": Docx2txtLoader,
                "file_types": [".docx","pdf"]
            },


        }

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æ¡£æ–‡æœ¬"""
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', ' ', text)
        # åˆå¹¶å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤æ–‡æ¡£å¤´å°¾çš„æ— å…³ä¿¡æ¯
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return ' '.join(lines[:1000])  # é™åˆ¶æœ€å¤§é•¿åº¦

    def load_category_documents(self, base_dir: str) -> List[Document]:
        """æŒ‰ç±»åˆ«åŠ è½½æ–‡æ¡£"""
        all_documents = []

        for category, config in self.category_config.items():
            category_dir = os.path.join(base_dir, category)
            if not os.path.exists(category_dir):
                print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {category_dir}")
                continue

            print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†ç±»åˆ«: {category}")

            for file_type in config["file_types"]:
                try:
                    loader = DirectoryLoader(
                        category_dir,
                        glob=f"**/*{file_type}",
                        loader_cls=config["loader"],
                        silent_errors=True
                    )
                    docs = loader.load()

                    for doc in docs:
                        # è·³è¿‡æ’é™¤æ–‡ä»¶
                        if any(exclude in doc.metadata['source'] for exclude in self.exclude_files):
                            continue

                        # æ¸…ç†å†…å®¹å¹¶å¢å¼ºå…ƒæ•°æ®
                        doc.page_content = self.clean_text(doc.page_content)
                        doc.metadata.update({
                            "category": category,
                            "source_name": os.path.basename(doc.metadata['source'])
                        })
                        all_documents.append(doc)
                        print(f"âœ… å·²åŠ è½½: {os.path.basename(doc.metadata['source'])}")

                except Exception as e:
                    print(f"âŒ åŠ è½½{category}ç±»æ–‡æ¡£å‡ºé”™: {str(e)}")
                    continue

        return all_documents

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """æ ¹æ®ç±»åˆ«ä½¿ç”¨ä¸åŒç­–ç•¥åˆ†å‰²æ–‡æ¡£"""
        split_docs = []

        for doc in documents:
            category = doc.metadata["category"]
            config = self.category_config.get(category, {})

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.get("chunk_size", 1000),
                chunk_overlap=config.get("chunk_overlap", 200),
                length_function=len
            )

            try:
                splits = splitter.split_documents([doc])
                for split in splits:
                    split.metadata.update(doc.metadata)  # ä¿ç•™åŸå§‹å…ƒæ•°æ®
                split_docs.extend(splits)
            except Exception as e:
                print(f"åˆ†å‰²æ–‡æ¡£å‡ºé”™: {str(e)}")
                continue

        return split_docs

    def train(self, docs_dir: str, db_dir: str):
        """è®­ç»ƒçŸ¥è¯†åº“"""
        print("ğŸ” å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•...")
        documents = self.load_category_documents(docs_dir)

        if not documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„")
            return False

        print(f"\nğŸ“‘ å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹åˆ†å‰²...")
        splits = self.split_documents(documents)
        print(f"âœ‚ï¸ åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")

        print("\nğŸ§  æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        try:
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=db_dir,
                collection_metadata={
                    "hnsw:space": "cosine",
                    "institution": "è¥¿åŒ—å¤§å­¦"
                }
            )
            vectorstore.persist()

            print(f"\nğŸ‰ çŸ¥è¯†åº“è®­ç»ƒå®Œæˆï¼")
            print(f"- æ–‡æ¡£ç±»åˆ«: {len(self.category_config)} ç±»")
            print(f"- å‘é‡å­˜å‚¨ä½ç½®: {db_dir}")
            return True

        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¥¿åŒ—å¤§å­¦çŸ¥è¯†åº“è®­ç»ƒç³»ç»Ÿ")
    parser.add_argument("--docs_dir", default="./æ•°æ®é›†", help="æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--db_dir", default="./nwu_knowledge_v2", help="å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„")

    args = parser.parse_args()

    if not os.path.exists(args.docs_dir):
        print(f"âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„...")
        os.makedirs(args.docs_dir)
        for category in ["åŸ¹å…»æ–¹æ¡ˆç›¸å…³", "æ—¥å¸¸ç”Ÿæ´»ç›¸å…³", "ç«èµ›ç›¸å…³",
                         "è¯¾ç¨‹ã€è€ƒè¯•èµ„æºç›¸å…³", "é€‰è¯¾è€ƒè¯•ç›¸å…³", "å­¦æ ¡æ¦‚å†µ"]:
            os.makedirs(os.path.join(args.docs_dir, category))
        print(f"âœ… å·²åˆ›å»ºç›®å½•ç»“æ„ï¼Œè¯·æŒ‰ç±»åˆ«æ”¾å…¥æ–‡æ¡£åé‡æ–°è¿è¡Œ")
        print(f"ç›®å½•ç»“æ„:\n{args.docs_dir}")
        for root, dirs, _ in os.walk(args.docs_dir):
            level = root.replace(args.docs_dir, '').count(os.sep)
            indent = ' ' * 4 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 4 * (level + 1)
            for d in dirs:
                print(f"{subindent}{d}/")
    else:
        trainer = NWUKnowledgeTrainer()
        success = trainer.train(args.docs_dir, args.db_dir)
        exit(0 if success else 1)