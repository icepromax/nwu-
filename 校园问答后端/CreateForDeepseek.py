import os
import re
from typing import List, Dict

from langchain.document_loaders import (
    PyPDFLoader,#.pdf
    Docx2txtLoader,#.docx
    UnstructuredFileLoader,
    DirectoryLoader,
    UnstructuredWordDocumentLoader,  # å¤„ç†.doc
    UnstructuredExcelLoader,  # å¤„ç†.xlsx
)

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter
)

from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
import argparse


class MultiFormatLoader:
    def __init__(self, file_path):
        self.file_path = file_path

    def load(self):
       try:
            if self.file_path.endswith('.pdf'):
                return PyPDFLoader(self.file_path).load()
            elif self.file_path.endswith('.docx'):
                return Docx2txtLoader(self.file_path).load()
            elif self.file_path.endswith('.doc'):
                return UnstructuredWordDocumentLoader(self.file_path).load()
            elif self.file_path.endswith('xlsx'):
                return UnstructuredExcelLoader(self.file_path).load()
            else:
                return UnstructuredFileLoader(self.file_path).load()
       except Exception as e:
           raise ValueError(f"Failed to load {self.file_path}: {str(e)}")
class NWUKnowledgeTrainer:
    def __init__(self):
        self.embeddings = OllamaEmbeddings(model="deepseek-r1:14b")
        self.exclude_files = ['.DS_Store', 'Thumbs.db']  # æ’é™¤ç³»ç»Ÿæ–‡ä»¶

        #å®šä¹‰å„ç›®å½•çš„å¤„ç†é…ç½®
        self.category_config = {
            "åŸ¹å…»æ–¹æ¡ˆç›¸å…³": {
                "chunk_size": 1500,
                "chunk_overlap": 400,
                "loader": MultiFormatLoader,
                "file_types": [".pdf"]
            },
            "æ—¥å¸¸ç”Ÿæ´»ç›¸å…³": {
                "chunk_size": 800,
                "chunk_overlap": 150,
                "loader": MultiFormatLoader,
                "file_types": [".docx", ".pdf"]
            },
            "ç«èµ›ç›¸å…³": {
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "loader": MultiFormatLoader,
                "file_types": [".pdf"]
            },
            "è¯¾ç¨‹ã€è€ƒè¯•èµ„æºç›¸å…³": {
                "chunk_size": 1200,
                "chunk_overlap": 300,
                "loader": MultiFormatLoader,
                "file_types": [".pdf", ".docx",".doc"]
            },
            "é€‰è¯¾è€ƒè¯•ç›¸å…³": {
                "chunk_size": 1000,
                "chunk_overlap": 250,
                "loader": MultiFormatLoader,
                "file_types": [".docx",".pdf",".doc",".xlsx"]
            },


        }

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æ¡£æ–‡æœ¬"""
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', ' ', text)
        # åˆå¹¶å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤æ–‡æ¡£å¤´å°¾çš„æ— å…³ä¿¡æ¯
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return ' '.join(lines[:1000])  # é™åˆ¶æœ€å¤§é•¿åº¦

    def load_category_documents(self, base_dir: str) -> List[Document]:
        """ä¿®å¤åçš„æ–‡æ¡£åŠ è½½æ–¹æ³•"""
        all_documents = []

        for category, config in self.category_config.items():
            category_dir = os.path.join(base_dir, category)
            if not os.path.exists(category_dir):
                print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {category_dir}")
                continue

            print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†ç±»åˆ«: {category}")

            for root, _, files in os.walk(category_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_ext = os.path.splitext(file)[1].lower()

                    # è¿‡æ»¤æ–‡ä»¶
                    if any(exclude in file for exclude in self.exclude_files):
                        continue
                    if file_ext not in config["file_types"]:
                        continue

                    # åŠ¨æ€é€‰æ‹©åŠ è½½å™¨
                    try:
                        if file_ext == '.pdf':
                            docs = PyPDFLoader(file_path).load()
                        elif file_ext == '.docx':
                            docs = Docx2txtLoader(file_path).load()
                        elif file_ext == '.doc':
                            docs = UnstructuredWordDocumentLoader(file_path).load()
                        elif file_ext == '.xlsx':
                            docs = UnstructuredExcelLoader(file_path).load()
                        else:
                            docs = UnstructuredFileLoader(file_path).load()

                        # æ¸…æ´—å’Œå¢å¼ºå…ƒæ•°æ®
                        for doc in docs:
                            doc.metadata.update({
                                "category": category,
                                "source_path": file_path
                            })
                            doc.page_content = self.clean_text(doc.page_content)
                            all_documents.append(doc)
                            print(f"âœ… å·²åŠ è½½: {file}")
                    except Exception as e:
                        print(f"âŒ åŠ è½½å¤±è´¥ {file}: {str(e)}")

        return all_documents

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """æ ¹æ®ç±»åˆ«ä½¿ç”¨ä¸åŒç­–ç•¥åˆ†å‰²æ–‡æ¡£"""
        split_docs = []

        for doc in documents:
            category = doc.metadata["category"]
            config = self.category_config.get(category, {})

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.get("chunk_size", 1000),
                chunk_overlap=config.get("chunk_overlap", 200),
                length_function=len
            )

            try:
                splits = splitter.split_documents([doc])
                for split in splits:
                    split.metadata.update(doc.metadata)  # ä¿ç•™åŸå§‹å…ƒæ•°æ®
                split_docs.extend(splits)
            except Exception as e:
                print(f"åˆ†å‰²æ–‡æ¡£å‡ºé”™: {str(e)}")
                continue

        return split_docs

    def train(self, docs_dir: str, db_dir: str):
        """è®­ç»ƒçŸ¥è¯†åº“"""
        print("ğŸ” å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•...")
        documents = self.load_category_documents(docs_dir)

        if not documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„")
            return False

        print(f"\nğŸ“‘ å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹åˆ†å‰²...")
        splits = self.split_documents(documents)
        print(f"âœ‚ï¸ åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")

        print("\nğŸ§  æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        try:
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=db_dir,
                collection_metadata={
                    "hnsw:space": "cosine",
                    "institution": "è¥¿åŒ—å¤§å­¦"
                }
            )
            vectorstore.persist()

            print(f"\nğŸ‰ çŸ¥è¯†åº“è®­ç»ƒå®Œæˆï¼")
            print(f"- æ–‡æ¡£ç±»åˆ«: {len(self.category_config)} ç±»")
            print(f"- å‘é‡å­˜å‚¨ä½ç½®: {db_dir}")
            return True

        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¥¿åŒ—å¤§å­¦çŸ¥è¯†åº“è®­ç»ƒç³»ç»Ÿ")
    parser.add_argument("--docs_dir", default="./æ•°æ®é›†", help="æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--db_dir", default="./nwu_knowledge_v1", help="å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„")

    args = parser.parse_args()

    if not os.path.exists(args.docs_dir):
        print(f"âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„...")
        os.makedirs(args.docs_dir)
        for category in ["åŸ¹å…»æ–¹æ¡ˆç›¸å…³", "æ—¥å¸¸ç”Ÿæ´»ç›¸å…³", "ç«èµ›ç›¸å…³",
                         "è¯¾ç¨‹ã€è€ƒè¯•èµ„æºç›¸å…³", "é€‰è¯¾è€ƒè¯•ç›¸å…³", "å­¦æ ¡æ¦‚å†µ"]:
            os.makedirs(os.path.join(args.docs_dir, category))
        print(f"âœ… å·²åˆ›å»ºç›®å½•ç»“æ„ï¼Œè¯·æŒ‰ç±»åˆ«æ”¾å…¥æ–‡æ¡£åé‡æ–°è¿è¡Œ")
        print(f"ç›®å½•ç»“æ„:\n{args.docs_dir}")
        for root, dirs, _ in os.walk(args.docs_dir):
            level = root.replace(args.docs_dir, '').count(os.sep)
            indent = ' ' * 4 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 4 * (level + 1)
            for d in dirs:
                print(f"{subindent}{d}/")
    else:
        trainer = NWUKnowledgeTrainer()
        success = trainer.train(args.docs_dir, args.db_dir)
        exit(0 if success else 1)